{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GretaGorzoni00/Assemblea_Costituzionale_NLP/blob/main/scr/roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VidezLOj8yQf",
        "outputId": "c85d44b4-eeb8-4167-c681-72e0274ac783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRMZ1vOt9bl-",
        "outputId": "2766ce94-302c-4a69-e710-5a57898d1d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/fine_tuning_roberta\n",
            "'Copia di Copia di Fine_Tuning.ipynb'   politicIT_phase_2_test_public.csv\n",
            "'Copia di Fine_Tuning.ipynb'            politicIT_phase_2_train.csv\n",
            " development.csv                        politicIT_phase_2_validation.csv\n",
            " evaluate_model-2.py                    politicIT_train.csv\n",
            " evaluate_model-4.py                    politicIT_validation.csv\n",
            " Fine_Tuning.ipynb                      predictions.csv\n",
            " \u001b[0m\u001b[01;34moutput\u001b[0m/                                roberta.ipynb\n",
            " \u001b[01;34moutput_roberta\u001b[0m/                        run_glue_no_trainer.py\n",
            " politicIT_phase_2_test_codalab.csv    'Split dataset.ipynb'\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/fine_tuning_roberta'\n",
        "%ls '/content/drive/MyDrive/fine_tuning_roberta'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzsrdqFA-WQT",
        "outputId": "1a4b477e-2127-4f39-d00c-9128d45591eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.47.1\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.1) (2025.6.15)\n",
            "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "Successfully installed transformers-4.47.1\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed evaluate-0.4.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.47.1\n",
        "!pip install datasets evaluate accelerate scikit-learn pandas numpy torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e36nDxXh9bz5",
        "outputId": "90eef80f-22b8-4f98-d7db-9b1b3f7f0966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 22:59:31.012522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750892371.048477    2118 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750892371.057883    2118 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-25 22:59:31.078361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "06/25/2025 22:59:36 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "Colonne nel train_df dopo pulizia: Index(['text', 'label'], dtype='object')\n",
            "Colonne nel valid_df dopo pulizia: Index(['text', 'label'], dtype='object')\n",
            "Class weights: tensor([0.9014, 1.1228])\n",
            "config.json: 100% 615/615 [00:00<00:00, 2.95MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"FacebookAI/xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 205kB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"FacebookAI/xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 76.3MB/s]\n",
            "tokenizer.json: 9.10MB [00:00, 89.1MB/s]\n",
            "loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/sentencepiece.bpe.model\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"FacebookAI/xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 1.12G/1.12G [00:12<00:00, 86.9MB/s]\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n",
            "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 93456/93456 [00:17<00:00, 5317.08 examples/s]\n",
            "Running tokenizer on dataset: 100% 10384/10384 [00:01<00:00, 6168.04 examples/s]\n",
            "06/25/2025 23:00:14 - INFO - __main__ - Sample 26087 of the training set: {'input_ids': [0, 2022, 2624, 4, 279, 133181, 96, 25, 234442, 378, 120148, 35522, 441, 34672, 268, 201715, 117, 73383, 378, 120148, 35522, 441, 34672, 268, 3725, 3720, 28, 19265, 5, 582, 10, 55630, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
            "06/25/2025 23:00:14 - INFO - __main__ - Sample 13762 of the training set: {'input_ids': [0, 1374, 65918, 115861, 5462, 290, 351, 66011, 7, 38938, 290, 51, 159, 17793, 184, 7134, 73143, 19081, 3224, 416, 351, 6912, 117, 221698, 18499, 14, 378, 120148, 35522, 441, 34672, 268, 30305, 32, 911, 565, 9720, 4104, 27340, 30780, 15019, 705, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
            "06/25/2025 23:00:14 - INFO - __main__ - Sample 39630 of the training set: {'input_ids': [0, 116, 47141, 7996, 891, 9071, 1864, 19138, 35890, 565, 9587, 2906, 13839, 4, 771, 587, 1526, 378, 120148, 35522, 441, 34672, 268, 2101, 377, 62809, 88955, 67, 5, 48923, 96, 26, 30610, 378, 120148, 35522, 441, 34672, 268, 78624, 40808, 290, 78, 565, 2053, 1300, 68300, 117, 351, 2060, 150, 116789, 160200, 118, 14763, 177340, 705, 51252, 14, 23, 76009, 378, 120148, 35522, 441, 34672, 268, 19252, 10, 197424, 378, 120148, 35522, 441, 34672, 268, 9244, 58745, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
            "Downloading builder script: 4.20kB [00:00, 13.1MB/s]\n",
            "06/25/2025 23:00:15 - INFO - __main__ - ***** Running training *****\n",
            "06/25/2025 23:00:15 - INFO - __main__ -   Num examples = 93456\n",
            "06/25/2025 23:00:15 - INFO - __main__ -   Num Epochs = 4\n",
            "06/25/2025 23:00:15 - INFO - __main__ -   Instantaneous batch size per device = 64\n",
            "06/25/2025 23:00:15 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "06/25/2025 23:00:15 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "06/25/2025 23:00:15 - INFO - __main__ -   Total optimization steps = 5844\n",
            "  0% 1/5844 [00:01<2:58:18,  1.83s/it]Step 0 - Loss: 0.7880548238754272\n",
            "  2% 101/5844 [01:46<1:39:24,  1.04s/it]Step 100 - Loss: 0.7295147180557251\n",
            "  3% 201/5844 [03:37<1:40:58,  1.07s/it]Step 200 - Loss: 0.6601155400276184\n",
            "  5% 301/5844 [05:26<1:36:55,  1.05s/it]Step 300 - Loss: 0.5793163180351257\n",
            "  7% 401/5844 [07:16<1:37:27,  1.07s/it]Step 400 - Loss: 0.6043080687522888\n",
            "  9% 501/5844 [09:06<1:36:23,  1.08s/it]Step 500 - Loss: 0.6095815300941467\n",
            " 10% 601/5844 [10:58<1:38:37,  1.13s/it]Step 600 - Loss: 0.6039587259292603\n",
            " 12% 701/5844 [12:49<1:33:44,  1.09s/it]Step 700 - Loss: 0.596427321434021\n",
            " 14% 801/5844 [14:40<1:31:59,  1.09s/it]Step 800 - Loss: 0.5672081708908081\n",
            " 15% 901/5844 [16:30<1:29:45,  1.09s/it]Step 900 - Loss: 0.6128407716751099\n",
            " 17% 1001/5844 [18:22<1:24:43,  1.05s/it]Step 1000 - Loss: 0.5495818853378296\n",
            " 19% 1101/5844 [20:13<1:31:08,  1.15s/it]Step 1100 - Loss: 0.5305018424987793\n",
            " 21% 1201/5844 [22:04<1:24:28,  1.09s/it]Step 1200 - Loss: 0.4767656922340393\n",
            " 22% 1301/5844 [23:55<1:22:27,  1.09s/it]Step 1300 - Loss: 0.565402626991272\n",
            " 24% 1401/5844 [25:45<1:19:31,  1.07s/it]Step 1400 - Loss: 0.46984490752220154\n",
            " 25% 1461/5844 [26:52<1:23:48,  1.15s/it]06/25/2025 23:28:02 - INFO - __main__ - epoch 0: {'accuracy': 0.7487480739599384}\n",
            " 25% 1462/5844 [27:47<20:55:05, 17.19s/it]Step 0 - Loss: 0.48450854420661926\n",
            " 27% 1562/5844 [29:38<1:18:06,  1.09s/it]Step 100 - Loss: 0.5733852982521057\n",
            " 28% 1662/5844 [31:29<1:15:26,  1.08s/it]Step 200 - Loss: 0.4765018820762634\n",
            " 30% 1762/5844 [33:20<1:18:39,  1.16s/it]Step 300 - Loss: 0.5488302111625671\n",
            " 32% 1862/5844 [35:10<1:13:00,  1.10s/it]Step 400 - Loss: 0.4729854166507721\n",
            " 34% 1962/5844 [37:01<1:09:14,  1.07s/it]Step 500 - Loss: 0.4596884250640869\n",
            " 35% 2062/5844 [38:51<1:05:58,  1.05s/it]Step 600 - Loss: 0.36587241291999817\n",
            " 37% 2162/5844 [40:43<1:12:43,  1.19s/it]Step 700 - Loss: 0.5792836546897888\n",
            " 39% 2262/5844 [42:34<1:04:44,  1.08s/it]Step 800 - Loss: 0.5008241534233093\n",
            " 40% 2362/5844 [44:24<1:02:10,  1.07s/it]Step 900 - Loss: 0.548462450504303\n",
            " 42% 2462/5844 [46:15<1:00:43,  1.08s/it]Step 1000 - Loss: 0.5266067385673523\n",
            " 44% 2562/5844 [48:05<1:00:43,  1.11s/it]Step 1100 - Loss: 0.48746198415756226\n",
            " 46% 2662/5844 [49:55<57:36,  1.09s/it]Step 1200 - Loss: 0.44312071800231934\n",
            " 47% 2762/5844 [51:45<52:55,  1.03s/it]Step 1300 - Loss: 0.5284033417701721\n",
            " 49% 2862/5844 [53:36<55:32,  1.12s/it]Step 1400 - Loss: 0.5726560354232788\n",
            " 50% 2922/5844 [54:42<53:02,  1.09s/it]06/25/2025 23:55:52 - INFO - __main__ - epoch 1: {'accuracy': 0.7697419106317411}\n",
            " 50% 2923/5844 [55:37<13:55:31, 17.16s/it]Step 0 - Loss: 0.4517175555229187\n",
            " 52% 3023/5844 [57:29<52:23,  1.11s/it]Step 100 - Loss: 0.3886029124259949\n",
            " 53% 3123/5844 [59:19<50:24,  1.11s/it]Step 200 - Loss: 0.38233137130737305\n",
            " 55% 3223/5844 [1:01:11<46:45,  1.07s/it]Step 300 - Loss: 0.4981100559234619\n",
            " 57% 3323/5844 [1:03:02<49:04,  1.17s/it]Step 400 - Loss: 0.4784833490848541\n",
            " 59% 3423/5844 [1:04:53<43:15,  1.07s/it]Step 500 - Loss: 0.41724199056625366\n",
            " 60% 3523/5844 [1:06:43<41:34,  1.07s/it]Step 600 - Loss: 0.4320444166660309\n",
            " 62% 3623/5844 [1:08:34<39:46,  1.07s/it]Step 700 - Loss: 0.510899007320404\n",
            " 64% 3723/5844 [1:10:24<39:21,  1.11s/it]Step 800 - Loss: 0.4228056073188782\n",
            " 65% 3823/5844 [1:12:15<35:36,  1.06s/it]Step 900 - Loss: 0.4107917249202728\n",
            " 67% 3923/5844 [1:14:05<36:28,  1.14s/it]Step 1000 - Loss: 0.29268985986709595\n",
            " 69% 4023/5844 [1:15:56<33:22,  1.10s/it]Step 1100 - Loss: 0.4669412076473236\n",
            " 71% 4123/5844 [1:17:47<31:49,  1.11s/it]Step 1200 - Loss: 0.2957149147987366\n",
            " 72% 4223/5844 [1:19:38<31:16,  1.16s/it]Step 1300 - Loss: 0.35347980260849\n",
            " 74% 4323/5844 [1:21:27<27:41,  1.09s/it]Step 1400 - Loss: 0.34845489263534546\n",
            " 75% 4383/5844 [1:22:32<27:19,  1.12s/it]06/26/2025 00:23:42 - INFO - __main__ - epoch 2: {'accuracy': 0.7809129429892142}\n",
            " 75% 4384/5844 [1:23:27<6:58:57, 17.22s/it]Step 0 - Loss: 0.28977683186531067\n",
            " 77% 4484/5844 [1:25:18<24:33,  1.08s/it]Step 100 - Loss: 0.39980122447013855\n",
            " 78% 4584/5844 [1:27:10<22:37,  1.08s/it]Step 200 - Loss: 0.35550132393836975\n",
            " 80% 4684/5844 [1:29:01<22:33,  1.17s/it]Step 300 - Loss: 0.40387821197509766\n",
            " 82% 4784/5844 [1:30:52<18:50,  1.07s/it]Step 400 - Loss: 0.23601406812667847\n",
            " 84% 4884/5844 [1:32:44<17:07,  1.07s/it]Step 500 - Loss: 0.49938663840293884\n",
            " 85% 4984/5844 [1:34:34<16:43,  1.17s/it]Step 600 - Loss: 0.31763407588005066\n",
            " 87% 5084/5844 [1:36:24<13:35,  1.07s/it]Step 700 - Loss: 0.23674078285694122\n",
            " 89% 5184/5844 [1:38:14<12:28,  1.13s/it]Step 800 - Loss: 0.3181353509426117\n",
            " 90% 5284/5844 [1:40:04<10:04,  1.08s/it]Step 900 - Loss: 0.34887245297431946\n",
            " 92% 5384/5844 [1:41:56<08:15,  1.08s/it]Step 1000 - Loss: 0.31643494963645935\n",
            " 94% 5484/5844 [1:43:47<06:28,  1.08s/it]Step 1100 - Loss: 0.3023514449596405\n",
            " 96% 5584/5844 [1:45:37<04:46,  1.10s/it]Step 1200 - Loss: 0.4428582191467285\n",
            " 97% 5684/5844 [1:47:29<02:57,  1.11s/it]Step 1300 - Loss: 0.2973795533180237\n",
            " 99% 5784/5844 [1:49:18<01:06,  1.11s/it]Step 1400 - Loss: 0.41167309880256653\n",
            "100% 5844/5844 [1:50:24<00:00,  1.05s/it]06/26/2025 00:51:34 - INFO - __main__ - epoch 3: {'accuracy': 0.7843798151001541}\n",
            "Configuration saved in output_roberta/config.json\n",
            "Model weights saved in output_roberta/model.safetensors\n",
            "tokenizer config file saved in output_roberta/tokenizer_config.json\n",
            "Special tokens file saved in output_roberta/special_tokens_map.json\n",
            "100% 5844/5844 [1:51:28<00:00,  1.14s/it]\n"
          ]
        }
      ],
      "source": [
        "!python run_glue_no_trainer.py \\\n",
        "  --model_name_or_path FacebookAI/xlm-roberta-base \\\n",
        "  --train_file politicIT_train.csv \\\n",
        "  --validation_file politicIT_validation.csv \\\n",
        "  --max_length 256 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 4 \\\n",
        "  --output_dir output_roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf_UHcU09dCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c64b68-f54e-4398-fabd-ea2260ded0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-26 01:51:08.326447: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750902668.346055    1438 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750902668.352166    1438 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-26 01:51:08.372191: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Map: 100% 36240/36240 [00:09<00:00, 3983.06 examples/s]\n",
            "\n",
            " predizioni salvate in: predictions.csv\n",
            "\n",
            " Metriche sul test set:\n",
            "Accuracy:  0.6990\n",
            "Precision: 0.6990\n",
            "Recall:    0.6990\n",
            "F1-score:  0.6990\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        left       0.73      0.72      0.73     19840\n",
            "       right       0.67      0.67      0.67     16400\n",
            "\n",
            "    accuracy                           0.70     36240\n",
            "   macro avg       0.70      0.70      0.70     36240\n",
            "weighted avg       0.70      0.70      0.70     36240\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python evaluate_model-4.py \\\n",
        "  --test_file politicIT_phase_2_test_codalab.csv \\\n",
        "  --model_path output_roberta \\\n",
        "  --batch_size 64"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}